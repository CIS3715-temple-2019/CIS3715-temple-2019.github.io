{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9: Twitter data analysis  \n",
    "\n",
    "In this lab, we will learn how to read JSON files and how to perform exploratory analysis of twitter data. As an extra credit, you will learn how to use API to download tweets on the topic of your choice.\n",
    "\n",
    "Let us start by importing the needed packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import time\n",
    "import calendar\n",
    "import codecs\n",
    "import datetime\n",
    "import sys\n",
    "import gzip\n",
    "import string\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import dateutil.parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 0: Reading Tweets\n",
    "\n",
    "Tweets are saved in the form of a JSON file. Open `onetweet` file in a text editor and study how it looks. This file contains information from a single tweet. The file is written in the JSON format, which is easy for a computer to read and parse. \n",
    "\n",
    "**Question 1**. Google `JSON` and try to learn about this particular data format. Explain in one or two paragraphs what you learned.\n",
    "\n",
    "Let us read `onetweet` JSON file. We will create an object and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "tweet = json.load(open('onetweet', 'rb'))\n",
    "pprint (tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, each tweet is stored in a dictionary. Although there are more than 20 different keys in the dictionary, we introduce the most important fields which will be used in the rest of this lab.\n",
    "<ul>\n",
    "<li>'_id': shows the unique id of this tweet.</li>\n",
    "\n",
    "<li>'coordinates': shows the location from which the tweet was posted. The field might be null if the tweet contains no location data, or it could contain bounding box information, place information, or GPS coordinates in the form of (longitude, latitude). </li>\n",
    "\n",
    "<li>'created_at': shows the time the tweet has been created.</li>\n",
    "\n",
    "<li>'text': shows the text of the tweet.</li>\n",
    "\n",
    "<li>'user': contains multiple dictionaries describing the user, including the name of this user, the number of followers, the number of friends...</li>\n",
    "</ul>\n",
    "\n",
    "**Question 2**. Explain 3 more keys that you find interesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code we will load around 4,000 tweets sent from New York City region during Sandy Hurricane from a JSON file `myNYC.json`. As part of this process, we will extract each tweet's post time and create a time series of the number of tweets in each hour during the event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filePath='smallNYC.json'\n",
    "localTweetList = []\n",
    "globalTweetCounter = 0\n",
    "frequencyMap = {}\n",
    "timeFormat = \"%a %b %d %H:%M:%S +0000 %Y\"\n",
    "reader = codecs.getreader(\"utf-8\")\n",
    "for line in open(filePath, 'rb'):\n",
    "    # Try to read tweet JSON into object\n",
    "    tweetObj = None\n",
    "    tweetObj = json.loads(reader.decode(line)[0])\n",
    "    \n",
    "    # Try to extract the time of the tweet\n",
    "    currentTime=dateutil.parser.parse(tweetObj['created_at'])\n",
    "    currentTime = currentTime.replace(second=0)\n",
    "    currentTime = currentTime.replace(minute=0)\n",
    "\n",
    "    #print currentTime\n",
    "    # Increment tweet count\n",
    "    globalTweetCounter += 1\n",
    "    \n",
    "    # If our frequency map already has this time, use it, otherwise add\n",
    "    if ( currentTime in frequencyMap.keys() ):\n",
    "        timeMap = frequencyMap[currentTime]\n",
    "        timeMap[\"count\"] += 1\n",
    "        timeMap[\"list\"].append(tweetObj)\n",
    "    else:\n",
    "        frequencyMap[currentTime] = {\"count\":1, \"list\":[tweetObj]}\n",
    "\n",
    "# Fill in any gaps\n",
    "times = sorted(frequencyMap.keys())\n",
    "firstTime = times[0]\n",
    "lastTime = times[-1]\n",
    "thisTime = firstTime\n",
    "\n",
    "#timeIntervalStep = datetime.timedelta(0, 60)    # Time step in seconds\n",
    "timeIntervalStep = datetime.timedelta(hours=1)\n",
    "while ( thisTime <= lastTime ):\n",
    "    if ( thisTime not in frequencyMap.keys() ):\n",
    "        frequencyMap[thisTime] = {\"count\":0, \"list\":[]}\n",
    "        \n",
    "    thisTime = thisTime + timeIntervalStep\n",
    "\n",
    "print (\"Processed Tweet Count:\", globalTweetCounter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 1: Simple Frequency Analysis\n",
    "\n",
    "In this section, we will cover a few simple analysis techniques for EDA of the available twitter datay.\n",
    "\n",
    "- Twitter Timeline\n",
    "- Top Twitter Users\n",
    "- Twitter API\n",
    "- Posting Frequency Distribution\n",
    "- Popular Hashtags\n",
    "- Simple Event Detection\n",
    "- Language Distributions\n",
    "\n",
    "### Twitter Timeline \n",
    "\n",
    "To build a timeline of Twitter usage, we can simply plot the number of tweets posted per hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(18.5,10.5)\n",
    "\n",
    "plt.title(\"Tweet Frequency\")\n",
    "\n",
    "# Sort the times into an array for future use\n",
    "sortedTimes = sorted(frequencyMap.keys())\n",
    "\n",
    "# What time span do these tweets cover?\n",
    "print (\"Time Frame:\", sortedTimes[0], sortedTimes[-1])\n",
    "\n",
    "# Get a count of tweets per minute\n",
    "postFreqList = [frequencyMap[x][\"count\"] for x in sortedTimes]\n",
    "\n",
    "# We'll have ticks every thirty minutes (much more clutters the graph)\n",
    "smallerXTicks = range(0, len(sortedTimes), 6)\n",
    "plt.xticks(smallerXTicks, [sortedTimes[x] for x in smallerXTicks], rotation=90)\n",
    "\n",
    "# Plot the post frequency\n",
    "ax.plot(range(len(frequencyMap)), [x if x > 0 else 0 for x in postFreqList], color=\"blue\", label=\"Posts\")\n",
    "ax.grid(b=True, which=u'major')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**. When is the tweeting activity the largest? Find a wikipedia article about the Sandy Hurricane to understand the timeline of events surrounding it. Discuss if you see a correlation with the tweet frequencies. Select some tweets from different time points and see if the messages are correlated with the events on the ground."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Twitter Users\n",
    "\n",
    "The following piece of code reveals the users that produced the most tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create maps for holding counts and tweets for each user\n",
    "globalUserCounter = {}\n",
    "globalUserMap = {}\n",
    "\n",
    "# Iterate through the time stamps\n",
    "for t in sortedTimes:\n",
    "    timeObj = frequencyMap[t]\n",
    "    \n",
    "    # For each tweet, pull the screen name and add it to the list\n",
    "    for tweet in timeObj[\"list\"]:\n",
    "        user = tweet[\"user\"][\"screen_name\"]\n",
    "        \n",
    "        if ( user not in globalUserCounter ):\n",
    "            globalUserCounter[user] = 1\n",
    "            globalUserMap[user] = [tweet]\n",
    "        else:\n",
    "            globalUserCounter[user] += 1\n",
    "            globalUserMap[user].append(tweet)\n",
    "\n",
    "print (\"Unique Users:\", len(globalUserCounter.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sortedUsers = sorted(globalUserCounter, key=globalUserCounter.get, reverse=True)\n",
    "print (\"Top Ten Most Prolific Users:\")\n",
    "for u in sortedUsers[:10]:\n",
    "    print (u, globalUserCounter[u], \"\\n\\t\", \"Random Tweet:\", globalUserMap[u][0][\"text\"], \"\\n----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**. Find and print the top 10 users with the most friends. Find and print the top 10 users with the most followers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Postings\n",
    "\n",
    "It appears a few users were posting to Twitter a lot. But how often did most Twitter users tweet during this time? We can build a histogram to see this distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "    \n",
    "# the histogram of the data\n",
    "plt.hist(\n",
    "    [globalUserCounter[x] for x in globalUserCounter], \n",
    "    bins=100, \n",
    "    normed=0, \n",
    "    alpha=0.75,\n",
    "    label=\"Counts\",\n",
    "    log=True)\n",
    "\n",
    "plt.xlabel('Number of Tweets')\n",
    "plt.ylabel('Counts')\n",
    "plt.title(\"Histogram of Frequency\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5**. Study 3 users with the most posts. What were they tweeting about?\n",
    "\n",
    "### Average Number of Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avgPostCount = np.mean([globalUserCounter[x] for x in globalUserCounter])\n",
    "print(\"Average Number of Posts:\", avgPostCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popular Hashtags\n",
    "\n",
    "Hashtags give us a quick way to view the conversation and see what people are discussing. Getting the most popular hashtags is just as easy as getting the most prolific users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A map for hashtag counts\n",
    "hashtagCounter = {}\n",
    "\n",
    "# For each minute, pull the list of hashtags and add to the counter\n",
    "for t in sortedTimes:\n",
    "    timeObj = frequencyMap[t]\n",
    "    \n",
    "    for tweet in timeObj[\"list\"]:\n",
    "        hashtagList = tweet[\"entities\"][\"hashtags\"]\n",
    "        \n",
    "        for hashtagObj in hashtagList:\n",
    "            \n",
    "            # We lowercase the hashtag to avoid duplicates (e.g., #MikeBrown vs. #mikebrown)\n",
    "            hashtagString = hashtagObj[\"text\"].lower()\n",
    "            \n",
    "            if ( hashtagString not in hashtagCounter ):\n",
    "                hashtagCounter[hashtagString] = 1\n",
    "            else:\n",
    "                hashtagCounter[hashtagString] += 1\n",
    "\n",
    "print (\"Unique Hashtags:\", len(hashtagCounter.keys()))\n",
    "sortedHashtags = sorted(hashtagCounter, key=hashtagCounter.get, reverse=True)\n",
    "print (\"Top Twenty Hashtags:\")\n",
    "for ht in sortedHashtags[:20]:\n",
    "    print (\"\\t\", \"#\" + ht, hashtagCounter[ht])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event Detection w/ Keyword Frequency\n",
    "\n",
    "Twitter is good for breaking news. When an impactful event occurs, we often see a spike on Twitter of the usage of a related keyword. Some examples are below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What keywords are we interested in?\n",
    "targetKeywords = [\"obama\", \"nyc\"]\n",
    "# targetKeywords.append(\"lowery\")\n",
    "# targetKeywords.append(\"reilly\")\n",
    "targetKeywords.append(\"sandy\")\n",
    "\n",
    "# Build an empty map for each keyword we are seaching for\n",
    "targetCounts = {x:[] for x in targetKeywords}\n",
    "totalCount = []\n",
    "\n",
    "# For each minute, pull the tweet text and search for the keywords we want\n",
    "for t in sortedTimes:\n",
    "    timeObj = frequencyMap[t]\n",
    "    \n",
    "    # Temporary counter for this minute\n",
    "    localTargetCounts = {x:0 for x in targetKeywords}\n",
    "    localTotalCount = 0\n",
    "    \n",
    "    for tweetObj in timeObj[\"list\"]:\n",
    "        tweetString = tweetObj[\"text\"].lower()\n",
    "\n",
    "        localTotalCount += 1\n",
    "        \n",
    "        # Add to the counter if the target keyword is in this tweet\n",
    "        for keyword in targetKeywords:\n",
    "            if ( keyword in tweetString ):\n",
    "                localTargetCounts[keyword] += 1\n",
    "                \n",
    "    # Add the counts for this minute to the main counter\n",
    "    totalCount.append(localTotalCount)\n",
    "    for keyword in targetKeywords:\n",
    "        targetCounts[keyword].append(localTargetCounts[keyword])\n",
    "        \n",
    "# Now plot the total frequency and frequency of each keyword\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(18.5,10.5)\n",
    "\n",
    "plt.title(\"Tweet Frequency\")\n",
    "plt.xticks(smallerXTicks, [sortedTimes[x] for x in smallerXTicks], rotation=90)\n",
    "\n",
    "ax.plot(range(len(frequencyMap)), totalCount, label=\"Total\")\n",
    "\n",
    "\n",
    "for keyword in targetKeywords:\n",
    "    ax.plot(range(len(frequencyMap)), targetCounts[keyword], label=keyword)\n",
    "ax.legend()\n",
    "ax.grid(b=True, which=u'major')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6**. Study some example codes with 2 different hashtags. Explain how are those messages different. \n",
    "\n",
    "**Question 7**. Which among the top 20 hashtags are related to Sandy. What are the most popular non-Sandy hashtags? \n",
    "\n",
    "**Question 8**. Among the top 20 hashtags, try to find which of them were particularly popular before Sandy, during Sandy, and those whose popularity did not change much. Try to apply k-means clustering on the time series of hashtags to try to find similar groups of hashtags. Discuss your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Distribution\n",
    "\n",
    "The following code gives an insight into the languages used for the tweets in your data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A map for counting each language\n",
    "languageCounter = {}\n",
    "\n",
    "for t in sortedTimes:\n",
    "    timeObj = frequencyMap[t]\n",
    "    \n",
    "    for tweet in timeObj[\"list\"]:\n",
    "        lang = tweet[\"lang\"]\n",
    "        \n",
    "        if ( lang not in languageCounter ):\n",
    "            languageCounter[lang] = 1\n",
    "        else:\n",
    "            languageCounter[lang] += 1\n",
    "            \n",
    "languages = sorted(languageCounter.keys(), key=languageCounter.get, reverse=True)\n",
    "\n",
    "for l in languages:\n",
    "    print (l, languageCounter[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "    \n",
    "# the histogram of the data\n",
    "plt.bar(\n",
    "    np.arange(len(languages)),\n",
    "    [languageCounter[x] for x in languages],\n",
    "    log=True)\n",
    "\n",
    "plt.xticks(np.arange(len(languages)) + 0.5, languages)\n",
    "plt.xlabel('Languages')\n",
    "plt.ylabel('Counts (Log)')\n",
    "plt.title(\"Language Frequency\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quesiton 9**. Now that you have experience in extracting different types of information from twitter data, perform your own EDA. Produce a 1-page report providing some interesting insights about the Sandy twitter data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 2: Classification\n",
    "\n",
    "In this part of the lab, we will work on a binary classificaiton problem. In particular, we would like to see how easy it is to discriminate between tweets with hashtag #sandy and the rest of them. To do it, we will first create labels: positive tweets will be those with hashtag #sandy and negative those without the hashtag. Then, we we will create a bag-of-words vector out of each tweet, but we will exclude word \"#sandy\". To produce the bag-of-words representation we will use the `CountVectorizer` functionality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs, json\n",
    "\n",
    "\"\"\"\n",
    "Step 1. Create sentences and labels from json file. \n",
    "\"\"\"\n",
    "filePath='smallNYC.json'\n",
    "sents = []\n",
    "reader = codecs.getreader(\"utf-8\")\n",
    "for line in open(filePath, 'rb'):\n",
    "    # Try to read tweet JSON into object\n",
    "    tweetObj = None\n",
    "    tweetObj = json.loads(reader.decode(line)[0])\n",
    "    sents.append(tweetObj['text'])\n",
    "    \n",
    "\"\"\"\n",
    "Step 2.1. Get label. If sent contains '#sandy', label=1; otherwise, label=0\n",
    "\"\"\"\n",
    "labels = np.array(['#sandy' in sent.split() for sent in sents], dtype='int')\n",
    "print(pd.Series(labels).value_counts())\n",
    "\"\"\"\n",
    "Step 2.2. Represent the data into Bag-of-words features, i.e, each sentence is a \n",
    "            vector of word counts. \n",
    "          a). Only select words with frequency >= 5\n",
    "          b). Remove label words '#sandy'\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=5, stop_words = ['#sandy'])\n",
    "features = vectorizer.fit_transform(sents)\n",
    "\n",
    "\n",
    "print('#sandy' in vectorizer.get_feature_names())\n",
    "print(features.toarray())\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quesiton 10**. Given the data set of labeled tweets, you have to train a classification model and check the accuracy. Remember to first split the data into training and test. You should explore kNN classification, decision tree classification, Random Forest classification.\n",
    "\n",
    "**Question 11**. Train the so-called *Logistic regression classifier* and check its accuracy. Compare with the results from *Question 10*. The following lines of code will be useful to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 3. Divide data into train and test \n",
    "\"\"\"\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.33, random_state=42) \n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = LogisticRegression()\n",
    "start_time = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "print('Time for %s fitting: %.3f' % ('LogisticRegression', time.time() - start_time))\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "y_pred_prob = clf.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print('Test Perf ACC: %.3f, AUC: %.3f' %(accuracy, auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3. Extra Credit: Using API to Download Tweets (50 extra points)\n",
    "\n",
    "To run the following code you will need to install the `tweepy` package.\n",
    "\n",
    "* Create a twitter account if you do not already have one\n",
    "* Go to https://apps.twitter.com/ and log in with your twitter credentials.\n",
    "* Click 'Create New App'\n",
    "* Fill out the Name, Description, Website fields and agree the terms. Put in any website you want if you don't have one you want to use.\n",
    "* On the next page, click the 'Keys and Access Tokens' tab along the top, then scroll all the way down until you see the section 'Your Access Token\n",
    "* Click the button 'Create My Access Token'.\n",
    "* You will now copy four values below. These values are your 'Consumer Key (API Key)', your 'Consumer Secret (API Secret)', your 'Access token' and your 'Access token secret'. All four should now be visible on the 'Keys and Access Tokens' page. Set the variables corresponding to the API key, API secret, access token, and access secret. You will see code like the below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import json, sys\n",
    "import codecs\n",
    "\n",
    "\n",
    "\n",
    "ckey = \"<Enter api key>\"\n",
    "csecret = \"<Enter api secret>\"\n",
    "atoken = \"<Enter your access token key here>\"\n",
    "asecret = \"<Enter your access token secret here>\"\n",
    "\n",
    "auth = OAuthHandler(ckey, csecret)\n",
    "auth.set_access_token(atoken, asecret)\n",
    "\n",
    "output = codecs.open('twitter_data.json', 'wb', encoding='utf-8')\n",
    "line_num = 0\n",
    "class CustomStreamListener(StreamListener):\n",
    "    def on_status(self, status):\n",
    "        print status.text\n",
    "\n",
    "    def on_data(self, data):\n",
    "        json_data = json.loads(data)\n",
    "        output.write(str(json_data))\n",
    "        output.write(\"\\n\")\n",
    "\n",
    "    def on_error(self, status_code):\n",
    "        print >> sys.stderr, 'Encountered error with status code:', status_code\n",
    "        return True # Don't kill the stream\n",
    "\n",
    "    def on_timeout(self):\n",
    "        print >> sys.stderr, 'Timeout...'\n",
    "        return True # Don't kill the stream\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following line of code allows you to download the current stream of codes that contain word *basketball*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitterStream = Stream(auth, CustomStreamListener())\n",
    "twitterStream.filter(track=['basketball'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can stop downloading by terminating the process (the stop button in Jupiter menu)\n",
    "\n",
    "Now you can also filter Tweets by locations by setting the parameters in filter.\n",
    "By setting the filter(locations = [-75.280291,39.867005,-74.955831,40.137959]), you will\n",
    "get all tweets from Philadelphia. To get bounding box for other areas, the klokantech's tool can\n",
    "be used. \n",
    "To learn more about Twitter Stream API parameters, please go to: \n",
    "    https://dev.twitter.com/streaming/overview/request-parameters \n",
    "    \n",
    "** Extra point Question**. Pick you favorite geographical region, keyword, or a hashtag and download at least 1,000 tweets. Perform any kind of exploratory or predictive data analysis that you find interesting and produce a 2-page report summarizing what you did and why and explaining what kind of results nd insights you obtained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
